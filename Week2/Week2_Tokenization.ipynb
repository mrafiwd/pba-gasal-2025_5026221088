{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization Techniques in NLP\n",
        "Week 2 NLP Pipeline\n",
        "PBA/ Genap 2025/ Irmasari Hafidz\n",
        "irma@its.ac.id\n",
        "\n",
        "## Install Dependencies\n"
      ],
      "metadata": {
        "id": "fNnKUG6g7ehT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install spacy sacremoses sentencepiece\n",
        "! python3 -m spacy download en_core_web_sm\n",
        "\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n"
      ],
      "metadata": {
        "id": "itgSTvy05_fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Required Libraries"
      ],
      "metadata": {
        "id": "LeQ6-9eg7qYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from sacremoses import MosesTokenizer\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "ya5NsMFj7uC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### White Space Tokenization\n",
        "*   one of the simplest tokenization techniques as it uses whitespace within the string as the delimiter of words.\n",
        "*   Wherever the white space is, it will split the data at that point\n",
        "*   Using **Python built** .split() in or **Spacy**\n",
        "\n",
        "```\n",
        "# sentence.split()\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IsbTVXDfIMId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Dikutip dari Reuters, dia menyampaikan permintaan maaf sebelum mengundurkan diri karena telah berlibur dengan keluarganya selama 4 minggu tak lama setelah banjir terjadi dan menewaskan lebih dari 100 orang.\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG6X83U4Ifgp",
        "outputId": "369d6187-64b4-4210-d430-4695ed7c9242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dikutip',\n",
              " 'dari',\n",
              " 'Reuters,',\n",
              " 'dia',\n",
              " 'menyampaikan',\n",
              " 'permintaan',\n",
              " 'maaf',\n",
              " 'sebelum',\n",
              " 'mengundurkan',\n",
              " 'diri',\n",
              " 'karena',\n",
              " 'telah',\n",
              " 'berlibur',\n",
              " 'dengan',\n",
              " 'keluarganya',\n",
              " 'selama',\n",
              " '4',\n",
              " 'minggu',\n",
              " 'tak',\n",
              " 'lama',\n",
              " 'setelah',\n",
              " 'banjir',\n",
              " 'terjadi',\n",
              " 'dan',\n",
              " 'menewaskan',\n",
              " 'lebih',\n",
              " 'dari',\n",
              " '100',\n",
              " 'orang.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "# Create a reference variable for Class WhitespaceTokenizer\n",
        "wtk = WhitespaceTokenizer()\n",
        "#give string input\n",
        "text1 = \"Computers offer powerful capabilities for searching and reasoning about structured records and relational data\"\n",
        "#use tokenize method\n",
        "tokens = wtk.tokenize(text1)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPwaegw0PUr7",
        "outputId": "eaea603b-7198-47ea-da44-88495680d791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Computers', 'offer', 'powerful', 'capabilities', 'for', 'searching', 'and', 'reasoning', 'about', 'structured', 'records', 'and', 'relational', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Tokenization** using Spacy and NLTK\n",
        "\n",
        "Word level Tokenization"
      ],
      "metadata": {
        "id": "dbYQoJsjJXo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Sample text for tokenization\n",
        "text = \"Menteri Urusan Keluarga di Jerman Anne Spiegel mengundurkan diri setelah kontroversinya berlibur pascabanjir dahsyat melanda Jerman pada 2021\"\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc= nlp(text)\n",
        "for token in doc:\n",
        "  print(token, token.idx)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Spacy Word Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "46ATNjPoCrtG",
        "outputId": "434a817f-63c2-4403-e844-603aa6ae9ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menteri 0\n",
            "Urusan 8\n",
            "Keluarga 15\n",
            "di 24\n",
            "Jerman 27\n",
            "Anne 34\n",
            "Spiegel 39\n",
            "mengundurkan 47\n",
            "diri 60\n",
            "setelah 65\n",
            "kontroversinya 73\n",
            "berlibur 88\n",
            "pascabanjir 97\n",
            "dahsyat 109\n",
            "melanda 117\n",
            "Jerman 125\n",
            "pada 132\n",
            "2021 137\n",
            "Spacy Word Tokens: ['Menteri', 'Urusan', 'Keluarga', 'di', 'Jerman', 'Anne', 'Spiegel', 'mengundurkan', 'diri', 'setelah', 'kontroversinya', 'berlibur', 'pascabanjir', 'dahsyat', 'melanda', 'Jerman', 'pada', '2021']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "tokens = word_tokenize(text)\n",
        "print(\"NLTK Word Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KYNZzOzJgrt",
        "outputId": "0d9bb431-e3bf-4f8b-e9df-2dde164970a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Word Tokens: ['Pengunduran', 'diri', 'tersebut', 'dilakukan', 'atas', 'keputusannya', 'sendiri', 'pada', '11', 'April', '2022', '.', 'Spiegel', 'diketahui', 'berlibur', 'setelah', 'bencana', 'banjir', 'terjadi', 'di', 'negara', 'bagian', 'tempat', 'dia', 'menjabat', 'sebagai', 'pejabat', 'senior', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import sent_tokenize from nltk library\n",
        "from nltk import sent_tokenize\n",
        "text = \"Good morning everyone. Welcome to the AI Workshop. Dr. Carter and Michael T. Anderson are waiting for you. They'll join you shortly.\"\n",
        "for t in sent_tokenize(text):\n",
        "\n",
        "    x =word_tokenize(t)\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkXvvHVFN76l",
        "outputId": "1f88d0da-2054-4302-a463-67f5b542f126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'morning', 'everyone', '.']\n",
            "['Welcome', 'to', 'the', 'AI', 'Workshop', '.']\n",
            "['Dr.', 'Carter', 'and', 'Michael', 'T.', 'Anderson', 'are', 'waiting', 'for', 'you', '.']\n",
            "['They', \"'ll\", 'join', 'you', 'shortly', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character Tokenization"
      ],
      "metadata": {
        "id": "j76ArJWbKEU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jerman\"\n",
        "characters = list(text)\n",
        "print(\"Character Tokens:\", characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veD0BOHwLDsC",
        "outputId": "4f10ff61-fb56-4ebb-ec9c-f7fd9eef912b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokens: ['J', 'e', 'r', 'm', 'a', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subword Tokenization\n",
        "\n",
        "*   breaks down words into smaller units called subwords.\n",
        "*   used to handle unknown or rare words by breaking them into small known words.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAZLfzCoLUvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "text = \"Public opposition to the urbanization of the countryside\"\n",
        "text2 = \"Tokenization is essential in NLP\"\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# Change 'tokenize.tokenize(text)' to 'tokenizer.tokenize(text)'\n",
        "subwords = tokenizer.tokenize(text)\n",
        "subwords2 = tokenizer.tokenize(text2)\n",
        "\n",
        "print(\"Bert Subword Tokens:\", subwords)\n",
        "print(\"Bert Subword Tokens:\", subwords2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cGSSmI_Lgcc",
        "outputId": "bfe3f7e5-374a-4e1c-b706-daa64d8dbdd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bert Subword Tokens: ['public', 'opposition', 'to', 'the', 'urban', '##ization', 'of', 'the', 'countryside']\n",
            "Bert Subword Tokens: ['token', '##ization', 'is', 'essential', 'in', 'nl', '##p']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11NTe4tEhHGv"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from sacremoses import MosesTokenizer\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Sample text for tokenization\n",
        "text = \"Pengunduran diri tersebut dilakukan atas keputusannya sendiri pada 11 April 2022. Spiegel diketahui berlibur setelah bencana banjir terjadi di negara bagian tempat dia menjabat sebagai pejabat senior.\"\n",
        "\n",
        "# White Space Tokenization\n",
        "def whitespace_tokenization(text):\n",
        "    return text.split()\n",
        "\n",
        "# Regular Expression Tokenizer\n",
        "def regex_tokenization(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "# Penn Treebank Tokenization\n",
        "def penn_treebank_tokenization(text):\n",
        "    tokenizer = TreebankWordTokenizer()\n",
        "    return tokenizer.tokenize(text)\n",
        "\n",
        "# SpaCy Tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def spacy_tokenization(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Moses Tokenization\n",
        "moses_tokenizer = MosesTokenizer()\n",
        "def moses_tokenization(text):\n",
        "    return moses_tokenizer.tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Display results\n",
        "print(\"White Space Tokenization:\", whitespace_tokenization(text))\n",
        "print(\"Regular Expression Tokenization:\", regex_tokenization(text))\n",
        "print(\"Penn Treebank Tokenization:\", penn_treebank_tokenization(text))\n",
        "print(\"SpaCy Tokenization:\", spacy_tokenization(text))\n",
        "print(\"Moses Tokenization:\", moses_tokenization(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFVtns2hFXLa",
        "outputId": "972124e0-e7c5-4fce-eb65-c7f9a72720ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "White Space Tokenization: ['Pengunduran', 'diri', 'tersebut', 'dilakukan', 'atas', 'keputusannya', 'sendiri', 'pada', '11', 'April', '2022.', 'Spiegel', 'diketahui', 'berlibur', 'setelah', 'bencana', 'banjir', 'terjadi', 'di', 'negara', 'bagian', 'tempat', 'dia', 'menjabat', 'sebagai', 'pejabat', 'senior.']\n",
            "Regular Expression Tokenization: ['Pengunduran', 'diri', 'tersebut', 'dilakukan', 'atas', 'keputusannya', 'sendiri', 'pada', '11', 'April', '2022', 'Spiegel', 'diketahui', 'berlibur', 'setelah', 'bencana', 'banjir', 'terjadi', 'di', 'negara', 'bagian', 'tempat', 'dia', 'menjabat', 'sebagai', 'pejabat', 'senior']\n",
            "Penn Treebank Tokenization: ['Pengunduran', 'diri', 'tersebut', 'dilakukan', 'atas', 'keputusannya', 'sendiri', 'pada', '11', 'April', '2022.', 'Spiegel', 'diketahui', 'berlibur', 'setelah', 'bencana', 'banjir', 'terjadi', 'di', 'negara', 'bagian', 'tempat', 'dia', 'menjabat', 'sebagai', 'pejabat', 'senior', '.']\n",
            "SpaCy Tokenization: ['Pengunduran', 'diri', 'tersebut', 'dilakukan', 'atas', 'keputusannya', 'sendiri', 'pada', '11', 'April', '2022', '.', 'Spiegel', 'diketahui', 'berlibur', 'setelah', 'bencana', 'banjir', 'terjadi', 'di', 'negara', 'bagian', 'tempat', 'dia', 'menjabat', 'sebagai', 'pejabat', 'senior', '.']\n",
            "Moses Tokenization: ['Pengunduran', 'diri', 'tersebut', 'dilakukan', 'atas', 'keputusannya', 'sendiri', 'pada', '11', 'April', '2022', '.', 'Spiegel', 'diketahui', 'berlibur', 'setelah', 'bencana', 'banjir', 'terjadi', 'di', 'negara', 'bagian', 'tempat', 'dia', 'menjabat', 'sebagai', 'pejabat', 'senior', '.']\n"
          ]
        }
      ]
    }
  ]
}